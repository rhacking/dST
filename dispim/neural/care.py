from typing import Dict, List, Union, Tuple, Any

import numpy as np
import pandas as pd
import tensorflow as tf
from csbdeep.data import RawData, create_patches
from csbdeep.models import Config, CARE
from scipy.ndimage import rotate

from .datagen import gen_test_image, degrade_image, gen_training_data


def gen_care_single_model(name: str) -> CARE:
    """
    Generate a single channel CARE model or retrieve the one that already exists under the name specified

    :param name: The name of the model
    :return: The CARE model
    """
    try:
        model = CARE(None, name)
    except FileNotFoundError:
        config = Config('xyzc', n_channel_in=1, n_channel_out=1)
        model = CARE(config, name)

    return model


def gen_care_dual_model(name: str, batch_size: int = 16, **kwargs):
    """
    Generate a dual channel CARE model or retrieve the one that already exists under the name specified

    :param name: The name of the model
    :param batch_size: The training batch size to use (only used if the model doesn't exist yet)
    :param kwargs: Parameters to pass to the model constructor (only used if the model doesn't exist yet
    :return: The CARE model
    """
    try:
        model = CARE(None, name)
    except FileNotFoundError:
        config = Config('xyzc', n_channel_in=2, n_channel_out=1, train_batch_size=batch_size, **kwargs)
        model = CARE(config, name)

    return model


def gen_raw_data(n: int, use_noise: bool, use_psf: bool, use_subsampling: bool, shape: int = 144):
    """
    Generate a csbdeep RawData object with random images generated by :func:`my text
    <dispim.neural.datagen.gen_training_data>`

    :param n:
    :param use_noise:
    :param use_psf:
    :param use_subsampling:
    :param shape:
    :return:
    """
    images_degr, images = gen_training_data(n, use_noise, use_psf, use_subsampling, shape=shape)
    return RawData.from_arrays(images_degr, images, axes='XYZC')


def split_chunks(data: List[np.ndarray], n_chunks: int) -> Tuple[np.ndarray, np.ndarray, str]:
    """
    Extract chunks from RawData and normalize them

    :param data: The data to extract the chunks form
    :param n_chunks: The number of chunks to extract
    :return: (X chunks, Y chunks, axes)
    """
    return create_patches(data, (64, 64, 64, 2), n_patches_per_image=n_chunks, patch_filter=None)


def train_care_generated_data(model: CARE, epochs: int, X: np.ndarray, Y: np.ndarray, X_val: np.ndarray,
                              Y_val: np.ndarray) -> None:
    """
    Train a CARE model on a dataset

    :param model: The CARE model to train
    :param epochs: The number of epochs to train for
    :param X: The training data input
    :param Y: The training data expected output
    :param X_val: The validation data input
    :param Y_val: The validation data expected output
    """
    rearr = lambda arr: np.moveaxis(arr, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])
    rearry = lambda arr: np.moveaxis(arr, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])[:, :, :, :, :1]
    X = rearr(X)
    X_val = rearr(X_val)
    Y = rearry(Y)
    Y_val = rearry(Y_val)

    model.train(X, Y, validation_data=(X_val, Y_val), epochs=epochs)


def test_model(model: CARE, x_test: np.ndarray, y_test: np.ndarray, batch_size: int = 4) -> Tuple[Any]:
    """
    Test the performance of a CARE model on test data

    :param model: The CARE model to test
    :param x_test: The test data input
    :param y_test: The test data expected output
    :param batch_size: The batch size to use when evaluating the model
    :return: The evaluation metrics as defined by the model (loss, followed by metrics)
    """
    rearr = lambda arr: np.moveaxis(arr, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])
    rearry = lambda arr: np.moveaxis(arr, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])[:, :, :, :, :1]
    X = rearr(x_test)
    Y = rearry(y_test)

    return model.keras_model.evaluate(X, Y, batch_size=batch_size)[1]


def evaluate_model(model, params: Dict[str, Union[List[float], np.ndarray]], shape: int,
                   use_noise: bool, use_psf: bool, use_subsampling: bool):
    """
    Analyze how the performance of a CARE model is affected by different parameters of degradation and image content

    :param model: The model to evaluate
    :param params: The parameters to test as a dictionary. The keys must be parameters to test and the values must
                    be values to test
    :param shape: The size of the test images generated to evaluate the performance of the model
    :param use_noise: Whether to use add noise to generated images
    :param use_psf: Whether to convolve the generated images with some PSF
    :param use_subsampling: Whether to downsample the generate images
    :return: A dictionary containing the results
    """
    from tqdm.auto import tqdm
    import warnings

    results = {}

    with warnings.catch_warnings():
        warnings.filterwarnings('ignore', module='scipy')
        warnings.filterwarnings('ignore', module='scikits.fitting')
        for param in tqdm(params, desc='Param'):
            param_results = _evaluate_param_influence(model, param, params[param], shape, use_noise, use_psf,
                                                      use_subsampling)
            results[param] = param_results

    return results


def plot_model_test_samples(model: CARE, x_test: np.ndarray, y_test: np.ndarray, n: int) -> None:
    """
    Plot examples of the performance of a dual-channel input CARE model

    :param model: The CARE model to test
    :param x_test: The test data input
    :param y_test: The test data expected output
    :param n: The number of examples to plot
    """
    import matplotlib.pyplot as plt

    rearr = lambda arr: np.moveaxis(arr, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])
    rearry = lambda arr: np.moveaxis(arr, [0, 1, 2, 3, 4], [0, 4, 1, 2, 3])[:, :, :, :, :1]
    X = rearr(x_test)
    if y_test.ndim < 5:
        Y = rearry(y_test[:, np.newaxis, :, :, :])
    else:
        Y = rearry(y_test)

    for i in range(n):
        id = np.random.randint(len(X))
        pred = model.keras_model.predict(X[id:id + 1, :, :, :, :])[0, :, :, :, 0]

        for ax in range(3):
            plt.figure(figsize=(15, 4))
            plt.suptitle(f'Axis {ax}')
            plt.subplot(141)
            plt.grid()
            plt.title('A')
            plt.imshow(np.sum(X[id, :, :, :, 0], axis=ax))

            plt.subplot(142)
            plt.grid()
            plt.title('B')
            plt.imshow(np.sum(X[id, :, :, :, 1], axis=ax))

            plt.subplot(143)
            plt.title('Prediction')
            plt.imshow(np.sum(pred, axis=ax))
            plt.grid()

            plt.subplot(144)
            plt.grid()
            plt.title('Ground truth')
            plt.imshow(np.sum(Y[id, :, :, :, 0], axis=ax))

            plt.show()

        print()


def _evaluate_param_influence(model: CARE, param: str, values: Union[List[float], np.ndarray], shape: int,
                              use_noise: bool, use_psf: bool, use_subsampling: bool):
    from scikits.fitting import GaussianFit
    from tqdm.auto import tqdm

    # FIXME: NORMALIZATION!

    psf_outer = int(shape // 4)
    psf_shape = shape - psf_outer * 2

    if param not in ['n_spheres', 'n_cylinders']:
        img = gen_test_image(shape=shape, n_spheres=70, n_cylinders=18)
        img_psf = img.copy()
        img_psf[psf_outer:-psf_outer, psf_outer:-psf_outer, psf_outer:-psf_outer] = 0
        img_psf[int(shape // 2), int(shape // 2), int(shape // 2)] = 1

    df = pd.DataFrame(columns=[param, 'mse', 'sigma_x', 'sigma_y', 'sigma_z'], dtype=np.float32)
    for v in tqdm(values, desc='Value', leave=False):
        for i in tqdm(range(8), leave=False, desc='Image'):
            success = False
            if param in ['n_spheres', 'n_cylinders']:
                params = {'n_spheres': 70, 'n_cylinders': 18}
                params[param] = v
                img = gen_test_image(shape=shape, **params)
                img_psf = img.copy()
                img_psf[psf_outer:-psf_outer, psf_outer:-psf_outer, psf_outer:-psf_outer] = 0
                img_psf[int(shape // 2), int(shape // 2), int(shape // 2)] = 1
            while not success:
                try:
                    params = {'psf_lateral_sigma': np.random.random() * 1.2 if use_psf else 0,
                              'psf_axial_sigma': np.random.random() * 5 if use_psf else 0,
                              'use_poisson': np.random.random() < 0.5 if use_noise else False,
                              'subsample_factor': np.random.random() * 0.5 + 0.5 if use_subsampling else 1.0,
                              'gauss_noise_sigma': np.random.random() * 0.42 if use_noise else 0.0}
                    if param in params.keys():
                        params[param] = v
                    elif param not in ['n_spheres', 'n_cylinders']:
                        raise ValueError(f'Unknown parameter: {param}')

                    degr = degrade_image(img, **params)
                    min_shape = np.min([degr.shape, img.shape], axis=0)
                    mse = np.mean(np.square(
                        img[:min_shape[0], :min_shape[1], :min_shape[2]] - model.keras_model.predict(
                            degr[np.newaxis, :min_shape[0], :min_shape[1], :min_shape[2]])[0, :, :, :, 0]))

                    degr_psf = degrade_image(img_psf, **params)
                    min_shape = np.min([degr_psf.shape, img_psf.shape], axis=0)
                    pred_psf = model.keras_model.predict(
                        degr_psf[np.newaxis, :min_shape[0], :min_shape[1], :min_shape[2]])[0, :, :, :, 0]
                    pred_psf = rotate(pred_psf, 45, (1, 2))

                    gauss = GaussianFit((int(psf_shape // 2), int(psf_shape // 2), int(psf_shape // 2)), (1, 1, 1), 1)
                    x = np.array(list(np.ndindex(psf_shape, psf_shape, psf_shape)))
                    y = [pred_psf[psf_outer:-psf_outer, psf_outer:-psf_outer, psf_outer:-psf_outer][
                             coord[0], coord[1], coord[2]] for coord in x]
                    gauss.fit(x.T, y)

                    df.loc[len(df)] = [v, mse, gauss.std[0], gauss.std[1], gauss.std[2]]
                    success = True
                except tf.errors.InvalidArgumentError:
                    pass

    return df
